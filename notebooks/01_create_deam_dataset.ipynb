{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "fb8e799c-ed8b-4485-89e9-cb0b62a4aa86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append('..')\n",
    "# import toolkits from src\n",
    "from src.make_dataset.deam_loader import * \n",
    "from src.make_dataset.lyric_utils import LyricsFetcher\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "ANNOTATIONS_PATH = Path(\"../data/raw/DEAM_Annotations/annotations/annotations averaged per song/song_level/static_annotations_averaged_songs_1_2000.csv\")\n",
    "METADATA_DIR = Path(\"../data/raw/metadata_DEAM/metadata\")\n",
    "TOKEN_FILE = Path(\"../secrets.txt\")\n",
    "CACHE_FILE = Path(\"../data/processed/DEAM/lyrics_cache.json\")\n",
    "OUTPUT_CORE = Path(\"../data/processed/core_dataset.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "3b2df106-5467-451e-9332-7c88c0fb6e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "from src import analysis_utils\n",
    "from src.make_dataset import deam_loader\n",
    "from src import aggregate\n",
    "from src.make_dataset import split_data\n",
    "\n",
    "reload(analysis_utils)\n",
    "reload(deam_loader)\n",
    "reload(aggregate)\n",
    "reload(split_data)\n",
    "from src.analysis_utils import *\n",
    "from src.make_dataset.deam_loader import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26707f6-fa9f-459d-9515-cb0ad043267e",
   "metadata": {},
   "source": [
    "### Create the base dataset (song id, artist name, track name, V-A mean and std columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61ea109c-7f63-4f0e-8bf7-2abfda793307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Creating base dataset \n",
      "Loaded 58 test songs\n",
      "Created dataset with 1802 songs\n",
      "  Train/val (â‰¤2000): 1744\n",
      "  Test (>2000): 58\n"
     ]
    }
   ],
   "source": [
    "# merge the different csvs together and the metadata on the matching song_id column\n",
    "print(\"Step 1: Creating base dataset \")\n",
    "base_df = create_deam_base_dataset(\n",
    "    annotations_path=ANNOTATIONS_PATH,\n",
    "    metadata_dir=METADATA_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fddbefa-144a-44cd-8034-c7f186ecad7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_df.to_parquet(OUTPUT_CORE)\n",
    "base_df.to_csv(\"../data/processed/core_dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5afec7-6cc8-4c6d-9cbb-0bd0bf0fcccb",
   "metadata": {},
   "source": [
    "### Add lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5eb2b90a-c5b7-474f-8b85-64b6a32aab4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-28 05:26:20,530 - INFO - LyricsFetcher initialized with fuzzy matching threshold of 85.\n",
      "2025-08-28 05:26:20,531 - INFO - Found 1802 songs needing lyrics.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 2: Enriching with lyrics\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "024ef7d5a2d1460a9bdeec54ccf0bdca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching Lyrics:   0%|          | 0/1802 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-28 05:26:20,588 - INFO - Lyrics fetching complete. Final cache saved.\n",
      "2025-08-28 05:26:20,589 - INFO - Found lyrics for 241 out of 1802 songs.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the lyrics fetcher and enrich the base dataset\n",
    "print(\"\\nStep 2: Enriching with lyrics\")\n",
    "token = TOKEN_FILE.read_text().strip()\n",
    "fetcher = LyricsFetcher(genius_api_token=token, cache_path=CACHE_FILE)\n",
    "final_df = fetcher.enrich_dataframe(base_df, batch_save_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2175b6ec-5b6b-471d-91b8-a791ac2cf30c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 3: Saving final dataset\n",
      "Successfully saved final dataset to ../data/processed/core_dataset_lyrics.parquet\n"
     ]
    }
   ],
   "source": [
    "# Save the dataset with lyrics\n",
    "print(\"\\nStep 3: Saving final dataset\")\n",
    "OUTPUT_LYRIC = Path(\"../data/processed/core_dataset_lyrics.parquet\")\n",
    "OUTPUT_LYRIC.parent.mkdir(parents=True, exist_ok=True)\n",
    "final_df.to_parquet(OUTPUT_LYRIC, index=False)\n",
    "print(f\"Successfully saved final dataset to {OUTPUT_LYRIC}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1df3e06-df52-4156-b74e-fd5425dc8208",
   "metadata": {},
   "source": [
    "#### Lyrics fetched with the Genius API via the lyricsgenius library\n",
    "#### Each (artist, track) was normalized (lowercasing, removing punctuation, trimming \"feat.\" and parentheses)\n",
    "#### Multiple name variants were tried for robust matching (e.g. \"and\" vs &, shortened artist names)\n",
    "#### Fuzzy string matching (rapidfuzz) was used when exact matches were not reliable (Threshold = 85)\n",
    "#### two-pass search strategy: first with artist and title, then title only with post-validation\n",
    "#### Retrieved lyrics were cleaned: removed ads, \"embed\", contributor info, or section headers. Short/empty results were discarded\n",
    "#### Cache file (lyrics_cache.json) ensured songs were not refetched and progress could resume if interrupted and avoid repeated API calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73192661-4a0c-488c-97d4-33cbb7be1f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1802 entries, 0 to 1801\n",
      "Data columns (total 8 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   song_id       1802 non-null   int64  \n",
      " 1   track_name    1802 non-null   object \n",
      " 2   artist_name   1802 non-null   object \n",
      " 3   valence_mean  1802 non-null   float64\n",
      " 4   arousal_mean  1802 non-null   float64\n",
      " 5   valence_std   1802 non-null   float64\n",
      " 6   arousal_std   1802 non-null   float64\n",
      " 7   lyrics        241 non-null    object \n",
      "dtypes: float64(4), int64(1), object(3)\n",
      "memory usage: 112.8+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>song_id</th>\n",
       "      <th>track_name</th>\n",
       "      <th>artist_name</th>\n",
       "      <th>valence_mean</th>\n",
       "      <th>arousal_mean</th>\n",
       "      <th>valence_std</th>\n",
       "      <th>arousal_std</th>\n",
       "      <th>lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Tonight  A Lonely Century</td>\n",
       "      <td>The New Mystikal Troubadours</td>\n",
       "      <td>3.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.63</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>DD Groove</td>\n",
       "      <td>Kevin MacLeod</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1.62</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>Slow Burn</td>\n",
       "      <td>Kevin MacLeod</td>\n",
       "      <td>5.7</td>\n",
       "      <td>5.5</td>\n",
       "      <td>1.42</td>\n",
       "      <td>1.63</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Nothing Much</td>\n",
       "      <td>My Bubba &amp; Mi</td>\n",
       "      <td>4.4</td>\n",
       "      <td>5.3</td>\n",
       "      <td>2.01</td>\n",
       "      <td>1.85</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>Hustle</td>\n",
       "      <td>Kevin MacLeod</td>\n",
       "      <td>5.8</td>\n",
       "      <td>6.4</td>\n",
       "      <td>1.47</td>\n",
       "      <td>1.69</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   song_id                 track_name                   artist_name  \\\n",
       "0        2  Tonight  A Lonely Century  The New Mystikal Troubadours   \n",
       "1        3                  DD Groove                 Kevin MacLeod   \n",
       "2        4                  Slow Burn                 Kevin MacLeod   \n",
       "3        5               Nothing Much                 My Bubba & Mi   \n",
       "4        7                     Hustle                 Kevin MacLeod   \n",
       "\n",
       "   valence_mean  arousal_mean  valence_std  arousal_std lyrics  \n",
       "0           3.1           3.0         0.94         0.63   None  \n",
       "1           3.5           3.3         1.75         1.62   None  \n",
       "2           5.7           5.5         1.42         1.63   None  \n",
       "3           4.4           5.3         2.01         1.85   None  \n",
       "4           5.8           6.4         1.47         1.69   None  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"\\nFinal DataFrame Info:\")\n",
    "final_df.info()\n",
    "display(final_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ce178e-7bb2-41b9-ac67-5a866802956a",
   "metadata": {},
   "source": [
    "### Save the final lyric dataset\n",
    "#### manual changes: addition of lyrics to niche songs from bandcamp and other less known lyric sources\n",
    "#### listening to each song that has lyrics to check for matching lyrics (deleting ones that didn't match)\n",
    "#### translating non-english songs with chatGPT4o with the prompt to preserve emotional conveyance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "809cc5a9-ce47-4dc7-9552-bc6bfba431b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated lyrics saved to: ../data/processed/core_dataset_lyrics.parquet and ../data/processed/core_dataset_lyrics.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "# Load data + curated cache\n",
    "final_df = pd.read_parquet(OUTPUT_LYRIC)\n",
    "with open(CACHE_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    cache = json.load(f)\n",
    "\n",
    "# Build keys exactly like the lyrics_cache file: \"artist|track\" lowercased\n",
    "keys = final_df[\"artist_name\"].str.lower() + \"|\" + final_df[\"track_name\"].str.lower()\n",
    "\n",
    "# Overwrite lyrics from cache to update the manual changes(NaN if no match)\n",
    "final_df[\"lyrics\"] = keys.map(cache)\n",
    "\n",
    "final_df.to_parquet(OUTPUT_LYRIC, index=False)\n",
    "\n",
    "# csv with utf-8-sig for proper display in Excel:\n",
    "final_df.to_csv(OUTPUT_LYRIC.with_suffix(\".csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"Updated lyrics saved to: {OUTPUT_LYRIC} and {OUTPUT_LYRIC.with_suffix('.csv')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9af0c926-e60f-4b68-b066-7ab9c51558d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1802 entries, 0 to 1801\n",
      "Data columns (total 8 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   song_id       1802 non-null   int64  \n",
      " 1   track_name    1802 non-null   object \n",
      " 2   artist_name   1802 non-null   object \n",
      " 3   valence_mean  1802 non-null   float64\n",
      " 4   arousal_mean  1802 non-null   float64\n",
      " 5   valence_std   1802 non-null   float64\n",
      " 6   arousal_std   1802 non-null   float64\n",
      " 7   lyrics        241 non-null    object \n",
      "dtypes: float64(4), int64(1), object(3)\n",
      "memory usage: 112.8+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>song_id</th>\n",
       "      <th>track_name</th>\n",
       "      <th>artist_name</th>\n",
       "      <th>valence_mean</th>\n",
       "      <th>arousal_mean</th>\n",
       "      <th>valence_std</th>\n",
       "      <th>arousal_std</th>\n",
       "      <th>lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Tonight  A Lonely Century</td>\n",
       "      <td>The New Mystikal Troubadours</td>\n",
       "      <td>3.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.63</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>DD Groove</td>\n",
       "      <td>Kevin MacLeod</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1.62</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>Slow Burn</td>\n",
       "      <td>Kevin MacLeod</td>\n",
       "      <td>5.7</td>\n",
       "      <td>5.5</td>\n",
       "      <td>1.42</td>\n",
       "      <td>1.63</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Nothing Much</td>\n",
       "      <td>My Bubba &amp; Mi</td>\n",
       "      <td>4.4</td>\n",
       "      <td>5.3</td>\n",
       "      <td>2.01</td>\n",
       "      <td>1.85</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>Hustle</td>\n",
       "      <td>Kevin MacLeod</td>\n",
       "      <td>5.8</td>\n",
       "      <td>6.4</td>\n",
       "      <td>1.47</td>\n",
       "      <td>1.69</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   song_id                 track_name                   artist_name  \\\n",
       "0        2  Tonight  A Lonely Century  The New Mystikal Troubadours   \n",
       "1        3                  DD Groove                 Kevin MacLeod   \n",
       "2        4                  Slow Burn                 Kevin MacLeod   \n",
       "3        5               Nothing Much                 My Bubba & Mi   \n",
       "4        7                     Hustle                 Kevin MacLeod   \n",
       "\n",
       "   valence_mean  arousal_mean  valence_std  arousal_std lyrics  \n",
       "0           3.1           3.0         0.94         0.63   None  \n",
       "1           3.5           3.3         1.75         1.62   None  \n",
       "2           5.7           5.5         1.42         1.63   None  \n",
       "3           4.4           5.3         2.01         1.85   None  \n",
       "4           5.8           6.4         1.47         1.69   None  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"\\nFinal DataFrame Info:\")\n",
    "final_df.info()\n",
    "display(final_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d9e7ef-83ec-4728-9346-7fd5d8c52c06",
   "metadata": {},
   "source": [
    "### Train/Val/Test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "82a415f9-a6c0-4605-acca-c98c53c0e3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.make_dataset.split_data import *\n",
    "core_df = pd.read_parquet(OUTPUT_CORE)\n",
    "core_df = core_df.set_index('song_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8f95fd26-b419-432d-a2a2-c9df29fed5a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Original ---\n",
      "Total: 1802\n",
      "Train: 1488 (82.6%) | Val: 256 (14.2%) | Test: 58 (3.2%)\n",
      "valence_mean: train Î¼=4.891, val Î¼=4.975, test Î¼=4.924\n",
      "arousal_mean: train Î¼=4.806, val Î¼=4.850, test Î¼=4.857\n",
      "Artist overlap | Tâˆ©V: 0, Tâˆ©Te: 0, Vâˆ©Te: 0\n"
     ]
    }
   ],
   "source": [
    "# original split creation\n",
    "tr, va, te = create_original_split(core_df, test_song_id_start=2001, val_size=0.15, random_state=42)\n",
    "analyze_split(core_df, tr, va, te, \"Original\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8c5200ec-109b-4d5f-b89b-1cf3a1f47dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_splits_triplet(tr, va, te, out_dir=\"../data/splits\", name=\"original\",\n",
    "                    meta={\"test_song_id_start\": 2001, \"val_size\": 0.15, \"random_state\": 42})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6c1b05ac-d96d-4473-9119-752e1e3352d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Augmented 70/15/15 ---\n",
      "Total: 1802\n",
      "Train: 1257 (69.8%) | Val: 276 (15.3%) | Test: 269 (14.9%)\n",
      "valence_mean: train Î¼=4.901, val Î¼=4.958, test Î¼=4.863\n",
      "arousal_mean: train Î¼=4.818, val Î¼=4.838, test Î¼=4.769\n",
      "Artist overlap | Tâˆ©V: 0, Tâˆ©Te: 0, Vâˆ©Te: 0\n"
     ]
    }
   ],
   "source": [
    "# Custom split to ~70/15/15\n",
    "tr2, va2, te2 = create_augmented_split(core_df, target_train=0.70, target_val=0.15, target_test=0.15,\n",
    "                                       test_song_id_start=2001, random_state=42)\n",
    "analyze_split(core_df, tr2, va2, te2, \"Augmented 70/15/15\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "9cc99b97-bcba-4e50-8083-1b8fe6909d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr2, va2, te2 = load_splits_triplet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "a9093278-d800-4b24-889a-79e85cafc733",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_splits_triplet(tr2, va2, te2, out_dir=\"../data/splits\", name=\"custom\",\n",
    "                    meta={\"targets\": [0.70, 0.15, 0.15], \"test_song_id_start\": 2001, \"random_state\": 42})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "9e05a4aa-0e59-4881-997d-a693532f6795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reusable K-folds\n",
    "dev_ids = tr2.union(va2) # train+val = development set (custom split)\n",
    "core_dev = core_df.loc[dev_ids].copy()\n",
    "folds_dev = create_kfold_splits(core_dev, n_splits=5, random_state=42) # don't leak test set\n",
    "\n",
    "save_kfold_splits(\n",
    "    folds_dev, out_dir=\"../data/splits\", name=\"cv5\",\n",
    "    meta={\"built_on\": \"custom_train+val\", \"n_splits\": 3, \"q\": 3, \"random_state\": 42}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "76724dd2-3fa0-4821-a6dc-c19f0326da7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artist overlap from dev to test: 0\n"
     ]
    }
   ],
   "source": [
    "from src.make_dataset.split_data import load_kfold_splits\n",
    "# 1) New folds donâ€™t touch test\n",
    "cv5_splits = load_kfold_splits()\n",
    "for i, (tr, va) in enumerate(cv5_splits):\n",
    "    assert tr.isin(te2).sum() == 0 and va.isin(te2).sum() == 0, f\"Fold {i} leaks test!\"\n",
    "\n",
    "# 2) Group leakage (artists) across splits\n",
    "def _artists(ids): return set(core_df.loc[ids, 'artist_name'].dropna())\n",
    "art_dev = _artists(dev_ids); art_te = _artists(te2)\n",
    "print(\"Artist overlap from dev to test:\", len(art_dev & art_te))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "1ebd312e-67f9-4230-ad34-c32e739f78d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Index([   2,    3,    4,    7,    8,   10,   12,   18,   19,   20,\n",
      "       ...\n",
      "       1987, 1988, 1991, 1992, 1993, 1994, 1995, 1997, 1998, 2000],\n",
      "      dtype='int64', name='song_id', length=1231), Index([  35,   44,   45,   62,   76,   79,   87,  102,  103,  105,\n",
      "       ...\n",
      "       1936, 1940, 1942, 1952, 1958, 1962, 1963, 1971, 1972, 1996],\n",
      "      dtype='int64', name='song_id', length=302))\n",
      "(Index([   2,    3,    4,    7,    8,   10,   12,   18,   19,   21,\n",
      "       ...\n",
      "       1983, 1984, 1985, 1986, 1987, 1991, 1993, 1994, 1995, 1996],\n",
      "      dtype='int64', name='song_id', length=1209), Index([  20,   39,   47,   50,   69,   72,   78,   82,   83,   85,\n",
      "       ...\n",
      "       1946, 1947, 1948, 1951, 1964, 1988, 1992, 1997, 1998, 2000],\n",
      "      dtype='int64', name='song_id', length=324))\n",
      "(Index([   3,    4,    7,    8,   10,   12,   19,   20,   21,   24,\n",
      "       ...\n",
      "       1985, 1986, 1988, 1992, 1994, 1995, 1996, 1997, 1998, 2000],\n",
      "      dtype='int64', name='song_id', length=1206), Index([   2,   18,   22,   32,   41,   42,   43,   48,   49,   51,\n",
      "       ...\n",
      "       1896, 1904, 1913, 1920, 1937, 1977, 1979, 1987, 1991, 1993],\n",
      "      dtype='int64', name='song_id', length=327))\n"
     ]
    }
   ],
   "source": [
    "print(cv5_splits[0])\n",
    "print(cv5_splits[1])\n",
    "print(cv5_splits[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "71b663c5-ff94-493c-8a9c-b811053d4d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_fold_bins(core_dev, splits, q=3, label_cols=(\"valence_mean\",\"arousal_mean\")):\n",
    "    df = core_dev[[*label_cols]].copy()\n",
    "    df[\"bins\"] = (pd.qcut(df[label_cols[0]], q=q, labels=False, duplicates='drop').astype('Int64').astype(str)\n",
    "                  + \"_\" +\n",
    "                  pd.qcut(df[label_cols[1]], q=q, labels=False, duplicates='drop').astype('Int64').astype(str))\n",
    "    all_bins = sorted(df[\"bins\"].dropna().unique())\n",
    "    bad = []\n",
    "    for i, (_, va) in enumerate(splits):\n",
    "        vc = df.loc[va, \"bins\"].value_counts()\n",
    "        missing = [b for b in all_bins if vc.get(b, 0) == 0]\n",
    "        if missing:\n",
    "            bad.append((i, missing))\n",
    "    return bad  # list of (fold_id, missing_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "0c58208d-de91-49a9-b13a-43cc281261de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_fold_bins(core_dev, cv5_splits, q=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "9bf3f23b-b29a-4a36-ba3b-356afc16cf69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, ['4_0']), (1, ['3_0']), (3, ['0_4'])]"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_fold_bins(core_dev, cv5_splits, q=5) # quantize on 3 std bins for proper V-A coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3abb694-bfa0-4969-9df9-7da8fa6b3082",
   "metadata": {},
   "source": [
    "### Add features to different dataset configurations and create hierarchy map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b715bbdd-8f1b-49ac-bad4-a88b8d9adbf2",
   "metadata": {},
   "source": [
    "Making 8 different possible dataset configurations:\n",
    "1. A dataset that contains the full 260 features and their 8 core stat descriptors (min, max, q25, q75, mean, std, kurtosis, skew) - 2080 total features (260*8)\n",
    "2. A dataset that contains the full 260 features and a rich 15 descriptor set that also includes trend, range, variation, median, etc. - 3900 total features (260*15)\n",
    "3. The 2nd dataset, but with decorrelation applied within each perceptual group - 2029\n",
    "4. The 1st dataset, but with decorrelation applied within each perceptual group - 1226\n",
    "5. 2nd dataset with global PCA applied (95% variance) - 2 PCAs\n",
    "6. 1st dataset with global PCA applied (95% variance) - 3 PCAs\n",
    "7. 2nd dataset, with PCA inside each perceptual group (95% variance) - 15 PCAs\n",
    "8. 1st dataset, with PCA inside each perceptual group (95% variance) - 48 PCAs\n",
    "- Because PCA is scale sensitive, the features used for datasets 5-8 will be only the custom split datasets, to reduce branching complexity of later experiments (have to fit the scalers + PCA on train split only before transforming all (train/val/test), and if both the original and custom splits are chosen for this, it's going to result in double the datasets considered - the original split will be evaluated anyway based on 20 different models to get compared with the custom split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "cbec29d1-ebe8-44ef-81d7-d48a484771ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES_PATH = Path(\"../data/raw/features\")\n",
    "OUTPUT_PATH = Path(\"../data/processed\")\n",
    "OUTPUT_PATH.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "ef45e6e5-948f-412e-bb4b-3337a9f3c5f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c3fb8610ce842c3b610c89fcc2aa571",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Aggregating features:   0%|          | 0/1802 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed: 1802 songs\n",
      "Failed: 0 songs\n"
     ]
    }
   ],
   "source": [
    "features_2080, failed_ids = create_features_2080(FEATURES_PATH)\n",
    "print(f\"Successfully processed: {len(features_2080)} songs\")\n",
    "print(f\"Failed: {len(failed_ids)} songs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "565850da-a4b4-4e2a-a759-29d73e3aa5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save a big set with all aggregated features and their rows(1802 rows x 260*8 columns (all features with suffix their descriptors))\n",
    "features_2080.to_parquet(OUTPUT_PATH / \"features_2080.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "f861b81e-88ec-4a09-ab89-3a2127486152",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_2080 = pd.read_parquet(OUTPUT_PATH / \"features_2080.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "7206924c-ebca-466f-b199-bda5bd0ecebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base dataset shape: (1802, 2080)\n",
      "First 5 columns: ['F0final_sma_stddev_mean', 'F0final_sma_stddev_std', 'F0final_sma_stddev_min', 'F0final_sma_stddev_max', 'F0final_sma_stddev_q25']\n",
      "Descriptor distribution: {'mean': 260, 'std': 260, 'min': 260, 'max': 260, 'q25': 260, 'q75': 260, 'skew': 260, 'kurtosis': 260}\n"
     ]
    }
   ],
   "source": [
    "from src.aggregate import *\n",
    "print(f\"Base dataset shape: {features_2080.shape}\")\n",
    "print(f\"First 5 columns: {list(features_2080.columns[:5])}\")\n",
    "print(f\"Descriptor distribution: {descriptor_distribution(features_2080.iloc[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "af4f132f-323d-4b77-8ab0-2fe0dbd8fa3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Songs in features: 1802\n",
      "Songs in labels: 1802\n",
      "Common songs: 1802\n",
      "Test songs in common: 58\n"
     ]
    }
   ],
   "source": [
    "# Test alignment with labels\n",
    "labels = pd.read_parquet(\"../data/processed/core_dataset.parquet\")\n",
    "labels = labels.set_index('song_id')\n",
    "        \n",
    "common_ids = features_2080.index.intersection(labels.index)\n",
    "print(f\"Songs in features: {len(features_2080)}\")\n",
    "print(f\"Songs in labels: {len(labels)}\")\n",
    "print(f\"Common songs: {len(common_ids)}\")\n",
    "print(f\"Test songs in common: {(common_ids > 2000).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "a88a3cf0-c3d7-4887-9600-ab3fb77ccd39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 unmapped features saved to ../results/unmapped_features.csv\n",
      "\n",
      "=== Feature Hierarchy Mapping Summary ===\n",
      "Total features: 260\n",
      "Mapped features: 228 (87.7%)\n",
      "Unmapped features: 32 (12.3%)\n",
      "\n",
      "=== Perceptual Dimension Distribution ===\n",
      "  melodiousness       :  16 cores\n",
      "  articulation        :  60 cores\n",
      "  rhythmic_stability  :  18 cores\n",
      "  rhythmic_complexity :  34 cores\n",
      "  dissonance          :  48 cores\n",
      "  tonal_stability     :  52 cores\n",
      "  unmapped            :  32 cores\n"
     ]
    }
   ],
   "source": [
    "# Build hierarchy map from aggregated features\n",
    "hierarchy = build_hierarchy_map(features_2080)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "1dc5829b-b75c-4c27-92ad-a743006e1412",
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchy.to_csv(\"../data/hierarchy_map.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "6eaac212-018d-4541-a2af-e405a57c76c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 1 - Full with 8 core descriptors\n",
      "Shape: (1802, 2080)\n"
     ]
    }
   ],
   "source": [
    "# Dataset 1: Keep 8 core descriptors and all features (baseline)\n",
    "features_2080.to_parquet(\"../data/processed/features_2080.parquet\")\n",
    "\n",
    "print(f\"Dataset 1 - Full with 8 core descriptors\")\n",
    "print(f\"Shape: {features_2080.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "2837ee3d-dc5a-417b-bc11-b74ffba7ff55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8464f946586f4639a161667ce7ce13f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Aggregating features:   0%|          | 0/1802 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed: 1802 songs\n",
      "Failed: 0 songs\n"
     ]
    }
   ],
   "source": [
    "# Dataset 2: save one that includes all of the available descriptors\n",
    "from src.aggregate import FULL\n",
    "features_3900, failed_ids = create_features_2080(FEATURES_PATH, descriptor_set=FULL)\n",
    "print(f\"Successfully processed: {len(features_3900)} songs\")\n",
    "print(f\"Failed: {len(failed_ids)} songs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "6f6565d3-0042-49e4-9853-9caa07302d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 2 - Full with 15 descriptors\n",
      "Shape: (1802, 3900)\n"
     ]
    }
   ],
   "source": [
    "features_3900.to_parquet(OUTPUT_PATH / \"features_3900.parquet\")\n",
    "print(f\"Dataset 2 - Full with 15 descriptors\")\n",
    "print(f\"Shape: {features_3900.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "fd455d3a-990e-43a3-a70a-f49f18eedcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_3900 = pd.read_parquet(OUTPUT_PATH / \"features_3900.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "4f50028b-75a2-4caf-80d5-69c7ef446709",
   "metadata": {},
   "outputs": [],
   "source": [
    "VALID_CATS = ['melodiousness','articulation','rhythmic_stability',\n",
    "              'rhythmic_complexity','dissonance','tonal_stability']\n",
    "hierarchy = hierarchy[hierarchy['perceptual'].isin(VALID_CATS)].copy() # exclude unmapped features from the reducted datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "e184e1c5-2db1-472b-9787-7fd113374099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perceptual\n",
      "articulation           60\n",
      "tonal_stability        52\n",
      "dissonance             48\n",
      "rhythmic_complexity    34\n",
      "rhythmic_stability     18\n",
      "melodiousness          16\n",
      "Name: count, dtype: int64\n",
      "core-like keys in map: 228\n"
     ]
    }
   ],
   "source": [
    "# 1) ensure no unmapped remains\n",
    "assert not (hierarchy['perceptual'] == 'unmapped').any()\n",
    "\n",
    "# 2) confirm categories present after your filter\n",
    "print(hierarchy['perceptual'].value_counts())\n",
    "\n",
    "# 3) confirm mapping keys match the function you use for lookups\n",
    "# if this prints zero, you were mixing base/core names\n",
    "keys_are_core = sum(hierarchy['feature'].str.contains(r'_(?:amean|stddev)$'))\n",
    "print(\"core-like keys in map:\", keys_are_core)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "9e543f82-f571-4bee-8f9d-9cfc45a89bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing 823 correlated features\n",
      "Dataset 3 - Decorrelated within groups(from 2080 set)\n",
      "Shape: (1802, 1257)\n",
      "Removed 823 correlated features\n",
      "  melodiousness       :   52 features remain\n",
      "  articulation        :  243 features remain\n",
      "  rhythmic_stability  :   82 features remain\n",
      "  rhythmic_complexity :  129 features remain\n",
      "  dissonance          :  184 features remain\n",
      "  tonal_stability     :  311 features remain\n"
     ]
    }
   ],
   "source": [
    "# Dataset 3: Remove highly correlated features within each perceptual group from the core 2080\n",
    "features_decorr_2080 = remove_correlated_within_groups(features_2080, hierarchy, threshold=0.95)\n",
    "features_decorr_2080.to_parquet(\"../data/processed/features_decorrelated_2080.parquet\")\n",
    "\n",
    "print(f\"Dataset 3 - Decorrelated within groups(from 2080 set)\")\n",
    "print(f\"Shape: {features_decorr_2080.shape}\")\n",
    "print(f\"Removed {features_2080.shape[1] - features_decorr_2080.shape[1]} correlated features\")\n",
    "\n",
    "# Show remaining features per group\n",
    "mapping = {row['feature']: row['perceptual'] for _, row in hierarchy.iterrows()}\n",
    "for group in ['melodiousness', 'articulation', 'rhythmic_stability', \n",
    "              'rhythmic_complexity', 'dissonance', 'tonal_stability']:\n",
    "    group_cols = [col for col in features_decorr_2080.columns \n",
    "                  if mapping.get(core_of(col)) == group]\n",
    "    print(f\"  {group:20s}: {len(group_cols):4d} features remain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "8837819c-572f-4f33-b82b-49888d42104e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing 1762 correlated features\n",
      "Dataset 4 - Decorrelated within groups (from 3900 set)\n",
      "Shape: (1802, 2138)\n",
      "Removed 1762 correlated features\n",
      "  melodiousness       :   87 features remain\n",
      "  articulation        :  396 features remain\n",
      "  rhythmic_stability  :  138 features remain\n",
      "  rhythmic_complexity :  223 features remain\n",
      "  dissonance          :  314 features remain\n",
      "  tonal_stability     :  500 features remain\n"
     ]
    }
   ],
   "source": [
    "# Dataset 4: Remove highly correlated features within each perceptual group\n",
    "features_decorr_3900 = remove_correlated_within_groups(features_3900, hierarchy, threshold=0.95)\n",
    "features_decorr_3900.to_parquet(\"../data/processed/features_decorrelated_3900.parquet\")\n",
    "\n",
    "print(f\"Dataset 4 - Decorrelated within groups (from 3900 set)\")\n",
    "print(f\"Shape: {features_decorr_3900.shape}\")\n",
    "print(f\"Removed {features_3900.shape[1] - features_decorr_3900.shape[1]} correlated features\")\n",
    "\n",
    "# Show remaining features per group\n",
    "mapping = {row['feature']: row['perceptual'] for _, row in hierarchy.iterrows()}\n",
    "for group in ['melodiousness', 'articulation', 'rhythmic_stability', \n",
    "              'rhythmic_complexity', 'dissonance', 'tonal_stability']:\n",
    "    group_cols = [col for col in features_decorr_3900.columns \n",
    "                  if mapping.get(core_of_full(col)) == group]\n",
    "    print(f\"  {group:20s}: {len(group_cols):4d} features remain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "3c9ec4da-bc6d-4cef-93a7-8fde1cccc7aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 5 - PCA per group on the 2080 set\n",
      "Shape: (1802, 347)\n",
      "\n",
      "Principal components per group:\n",
      "  melodiousness       : 24 PCs\n",
      "  articulation        : 83 PCs\n",
      "  rhythmic_stability  : 31 PCs\n",
      "  rhythmic_complexity : 41 PCs\n",
      "  dissonance          : 56 PCs\n",
      "  tonal_stability     : 112 PCs\n"
     ]
    }
   ],
   "source": [
    "# Dataset 5: PCA within each perceptual group on core 2080 features(preserves interpretability)\n",
    "features_pca_grouped_2080, pca_models = pca_per_group(features_decorr_2080, hierarchy, train_ids=tr2, variance_explained=0.95)\n",
    "features_pca_grouped_2080.to_parquet(\"../data/processed/features_pca_per_group_2080.parquet\")\n",
    "\n",
    "print(f\"Dataset 5 - PCA per group on the 2080 set\")\n",
    "print(f\"Shape: {features_pca_grouped_2080.shape}\")\n",
    "\n",
    "# Show PCs per group\n",
    "print(\"\\nPrincipal components per group:\")\n",
    "for group in ['melodiousness', 'articulation', 'rhythmic_stability',\n",
    "              'rhythmic_complexity', 'dissonance', 'tonal_stability']:\n",
    "    group_pcs = [col for col in features_pca_grouped_2080.columns if col.startswith(group)]\n",
    "    if group_pcs:\n",
    "        print(f\"  {group:20s}: {len(group_pcs):2d} PCs\")\n",
    "\n",
    "# Save PCA models for interpretation\n",
    "with open(\"../data/pca_pickles/pca_per_group_models_2080.pkl\", 'wb') as f:\n",
    "    pickle.dump(pca_models, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "18c26f4a-f24c-40ee-8cc7-37fafd25f7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 6 - PCA per group on the 3900 set\n",
      "Shape: (1802, 516)\n",
      "\n",
      "Principal components per group:\n",
      "  melodiousness       : 32 PCs\n",
      "  articulation        : 125 PCs\n",
      "  rhythmic_stability  : 45 PCs\n",
      "  rhythmic_complexity : 66 PCs\n",
      "  dissonance          : 87 PCs\n",
      "  tonal_stability     : 161 PCs\n"
     ]
    }
   ],
   "source": [
    "# Dataset 6: PCA within each perceptual group on full 3900 features(preserves interpretability)\n",
    "features_pca_grouped_3900, pca_models = pca_per_group(features_decorr_3900, hierarchy, train_ids=tr2, variance_explained=0.95)\n",
    "features_pca_grouped_3900.to_parquet(\"../data/processed/features_pca_per_group_3900.parquet\")\n",
    "\n",
    "print(f\"Dataset 6 - PCA per group on the 3900 set\")\n",
    "print(f\"Shape: {features_pca_grouped_3900.shape}\")\n",
    "\n",
    "# Show PCs per group\n",
    "print(\"\\nPrincipal components per group:\")\n",
    "for group in ['melodiousness', 'articulation', 'rhythmic_stability',\n",
    "              'rhythmic_complexity', 'dissonance', 'tonal_stability']:\n",
    "    group_pcs = [col for col in features_pca_grouped_3900.columns if col.startswith(group)]\n",
    "    if group_pcs:\n",
    "        print(f\"  {group:20s}: {len(group_pcs):2d} PCs\")\n",
    "\n",
    "# Save PCA models for interpretation\n",
    "with open(\"../data/pca_pickles/pca_per_group_models_3900.pkl\", 'wb') as f:\n",
    "    pickle.dump(pca_models, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "03f0e949-0a42-4b9e-a5ea-e307f05ea8df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 7 - Global PCA on 2080 features with 95% variance retainment\n",
      "Shape: (1802, 242)\n",
      "Explained variance: 0.950\n",
      "First 10 PCs explain: 0.585\n"
     ]
    }
   ],
   "source": [
    "# Dataset 7: Global PCA on full feature set (2080), only on train_ids (custom split)\n",
    "scaler_2080 = StandardScaler().fit(features_2080.loc[tr2])\n",
    "# scale on train only\n",
    "X_train_scaled = scaler_2080.transform(features_2080.loc[tr2])\n",
    "X_all_scaled   = scaler_2080.transform(features_2080)\n",
    "# PCA on scaled train - 95% var\n",
    "pca_global_2080 = PCA(n_components=0.95, random_state=42).fit(X_train_scaled)\n",
    "\n",
    "X_all_pca_2080 = pca_global_2080.transform(X_all_scaled)\n",
    "features_pca_global_2080 = pd.DataFrame(\n",
    "    X_all_pca_2080,\n",
    "    index=features_2080.index,\n",
    "    columns=[f\"global_PC{i+1}\" for i in range(pca_global_2080.n_components_)]\n",
    ")\n",
    "features_pca_global_2080.to_parquet(\"../data/processed/features_pca_global_2080.parquet\")\n",
    "\n",
    "print(f\"Dataset 7 - Global PCA on 2080 features with 95% variance retainment\")\n",
    "print(f\"Shape: {features_pca_global_2080.shape}\")\n",
    "print(f\"Explained variance: {pca_global_2080.explained_variance_ratio_.sum():.3f}\")\n",
    "print(f\"First 10 PCs explain: {pca_global_2080.explained_variance_ratio_[:10].sum():.3f}\")\n",
    "\n",
    "# Save PCA model for later use\n",
    "import pickle\n",
    "with open(\"../data/pca_pickles/pca_global_model_2080.pkl\", 'wb') as f:\n",
    "    pickle.dump(pca_global_2080, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "5daedde9-dc0a-4908-a2be-26c621714412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 8 - Global PCA on 3900 features with 95% variance retainment\n",
      "Shape: (1802, 317)\n",
      "Explained variance: 0.950\n",
      "First 10 PCs explain: 0.551\n"
     ]
    }
   ],
   "source": [
    "# Dataset 8: Global PCA on full feature set (3900), only on train_ids (custom split)\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "scaler_3900 = StandardScaler().fit(features_3900.loc[tr2])\n",
    "# scale on train only\n",
    "X_train_scaled = scaler_3900.transform(features_3900.loc[tr2])\n",
    "X_all_scaled   = scaler_3900.transform(features_3900)\n",
    "# PCA on scaled train - 95% var\n",
    "pca_global_3900 = PCA(n_components=0.95, random_state=42).fit(X_train_scaled)\n",
    "\n",
    "X_all_pca_3900 = pca_global_3900.transform(X_all_scaled)\n",
    "features_pca_global_3900 = pd.DataFrame(\n",
    "    X_all_pca_3900,\n",
    "    index=features_3900.index,\n",
    "    columns=[f\"global_PC{i+1}\" for i in range(pca_global_3900.n_components_)]\n",
    ")\n",
    "features_pca_global_3900.to_parquet(\"../data/processed/features_pca_global_3900.parquet\")\n",
    "\n",
    "print(f\"Dataset 8 - Global PCA on 3900 features with 95% variance retainment\")\n",
    "print(f\"Shape: {features_pca_global_3900.shape}\")\n",
    "print(f\"Explained variance: {pca_global_3900.explained_variance_ratio_.sum():.3f}\")\n",
    "print(f\"First 10 PCs explain: {pca_global_3900.explained_variance_ratio_[:10].sum():.3f}\")\n",
    "\n",
    "# Save PCA model for later use\n",
    "import pickle\n",
    "with open(\"../data/pca_pickles/pca_global_model_3900.pkl\", 'wb') as f:\n",
    "    pickle.dump(pca_global_3900, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "729f619e-3a3a-4844-bf48-c664a9824077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All datasets properly aligned with labels\n"
     ]
    }
   ],
   "source": [
    "# check that all 8 datasets align with labels\n",
    "for path in ['features_2080.parquet', 'features_3900.parquet', \n",
    "             'features_decorrelated_3900.parquet', 'features_decorrelated_2080.parquet',\n",
    "             'features_pca_global_3900.parquet', 'features_pca_global_2080.parquet',\n",
    "             'features_pca_per_group_3900.parquet', 'features_pca_per_group_2080.parquet']:\n",
    "    df = pd.read_parquet(f\"../data/processed/{path}\")\n",
    "    common = df.index.intersection(labels.index)\n",
    "    assert len(common) == 1802, f\"Alignment issue with {path}\"\n",
    "    \n",
    "print(\"All datasets properly aligned with labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda86421-d4a5-41ba-bed0-a804dce43e11",
   "metadata": {},
   "source": [
    "### Create a deep learning version of the dataset (for RNN sequential modelling instead of stat descriptors derived from rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "11a9b29d-9176-4067-b4d6-57010c1355f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "from src.make_dataset import deam_loader\n",
    "\n",
    "reload(deam_loader)\n",
    "from src.make_dataset.deam_loader import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ce13a1a0-763e-40c7-b282-2e1c01f182a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub-sample 90 rows only from each song to have a fair input size across songs\n",
    "rnn_array, rnn_ids = create_rnn_dataset(FEATURES_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc7a084-cb4c-4e67-b6c9-30771acbf6cc",
   "metadata": {},
   "source": [
    "### calculate key, mode and bpm and create a separate dataframe with them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fffde8c2-ab03-4730-b1e0-6794852a5da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Paths\n",
    "core_df = pd.read_parquet(\"../data/processed/core_dataset.parquet\")\n",
    "AUDIO_DIR = Path(\"../data/raw/audio_files_DEAM/MEMD_audio\")\n",
    "OUTPUT_PATH = Path(\"../data/processed/audio_metadata.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7deac759-c1af-4312-8a0b-f10adbd9b86d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'src.make_dataset.bpm_key_mode' from '/home/georgios/PGMP/notebooks/../src/make_dataset/bpm_key_mode.py'>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "from src.make_dataset import bpm_key_mode\n",
    "\n",
    "reload(bpm_key_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f7660f61-7c10-41fe-aac7-fce2573cb464",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Estimating BPM, Key, and Mode: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1802/1802 [46:43<00:00,  1.56s/it]\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "from tqdm import tqdm\n",
    "from src.make_dataset.bpm_key_mode import *\n",
    "\n",
    "# Get Song IDs\n",
    "all_song_ids = core_df['song_id'].unique()\n",
    "\n",
    "# Process Audio Files\n",
    "records = []\n",
    "for song_id in tqdm(all_song_ids, desc=\"Estimating BPM, Key, and Mode\"):\n",
    "    audio_file = AUDIO_DIR / f\"{song_id}.mp3\"\n",
    "    if not audio_file.exists():\n",
    "        continue\n",
    "        \n",
    "    key, mode, confidence = estimate_key_with_confidence(audio_file)\n",
    "    bpm = estimate_bpm(audio_file)\n",
    "    \n",
    "    records.append({'song_id': song_id, 'key': key, 'mode': mode, 'key_confidence': confidence, 'bpm': bpm})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "58e14df6-7ee4-4a80-b7c8-3526a6c4799f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata DataFrame created with 1802 records.\n",
      "   song_id key   mode  key_confidence     bpm\n",
      "0        2   A  minor        0.259305  143.55\n",
      "1        3   E  minor        0.706457   95.70\n",
      "2        4   A  minor        0.983184   86.13\n",
      "3        5   B  major        0.997981   99.38\n",
      "4        7   C  major        0.778527  117.45\n"
     ]
    }
   ],
   "source": [
    "# Create and Save DataFrame\n",
    "metadata_df = pd.DataFrame(records)\n",
    "metadata_df.to_parquet(OUTPUT_PATH, index=False)\n",
    "\n",
    "print(f\"Metadata DataFrame created with {len(metadata_df)} records.\")\n",
    "print(metadata_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2748c8e1-4d9a-48e6-83f8-9a8b67e122fe",
   "metadata": {},
   "source": [
    "## map creation vol_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd04a95-8843-4ed3-b6f5-722629de5813",
   "metadata": {},
   "source": [
    "Second attempt at mapping the features into 3 categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "319e05a4-602f-4328-83e8-c33231732b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acoustic Feature                    | Proposed Perceptual Category\n",
      "--------------------------------------------------------------------\n",
      "F0final_sma_amean                   | melodiousness\n",
      "F0final_sma_de_stddev               | tonal_stability\n",
      "audSpec_Rfilt_sma_de[0]_amean       | rhythmic_complexity\n",
      "audSpec_Rfilt_sma_de[10]_amean      | articulation\n",
      "logHNR_sma_de_stddev                | dissonance\n",
      "pcm_RMSenergy_sma_amean             | rhythmic_stability\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../data/openSMILE_Perceptual_mapping__draft_.csv')\n",
    "\n",
    "filtered_df = df[df['Confidence'].isin(['High', 'Medium'])]\n",
    "\n",
    "result_df = filtered_df[['Acoustic feature', 'Proposed perceptual category']]\n",
    "\n",
    "# Print the selected columns side-by-side\n",
    "print(\"Acoustic Feature                    | Proposed Perceptual Category\")\n",
    "print(\"--------------------------------------------------------------------\")\n",
    "#print 1 of each\n",
    "seen = set()\n",
    "for index, row in result_df.iterrows():\n",
    "    if row['Proposed perceptual category'] in seen:\n",
    "        continue\n",
    "    print(f\"{row['Acoustic feature']:<35} | {row['Proposed perceptual category']}\")\n",
    "    seen.add(row['Proposed perceptual category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a3a236-15e9-47d4-9748-c7ae097e2d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in result_df.iterrows():\n",
    "    print(f\"{row['Acoustic feature']:<35} | {row['Proposed perceptual category']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3859dc85-a7c7-49ee-b1bf-2e38584924aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of each unique perceptual category ('High' & 'Medium' Confidence):\n",
      "\n",
      "Proposed perceptual category\n",
      "articulation           48\n",
      "rhythmic_complexity    30\n",
      "dissonance             16\n",
      "tonal_stability         8\n",
      "melodiousness           4\n",
      "rhythmic_stability      2\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count the occurrences of each unique value in the specified column\n",
    "perceptual_counts = filtered_df['Proposed perceptual category'].value_counts()\n",
    "\n",
    "print(\"Sum of each unique perceptual category ('High' & 'Medium' Confidence):\\n\")\n",
    "print(perceptual_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "83d85443-45df-4d2b-b00a-d2b4e470ce89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152 unmapped features saved to ../results/unmapped_features.csv\n",
      "\n",
      "=== Feature Hierarchy Mapping Summary ===\n",
      "Total features: 260\n",
      "Mapped features: 108 (41.5%)\n",
      "Unmapped features: 152 (58.5%)\n",
      "\n",
      "=== Perceptual Dimension Distribution ===\n",
      "  articulation        :  48 cores\n",
      "  rhythmic_complexity :  30 cores\n",
      "  dissonance          :  16 cores\n",
      "  tonal_stability     :   8 cores\n",
      "  melodiousness       :   4 cores\n",
      "  rhythmic_stability  :   2 cores\n",
      "  minorness           :   0 cores\n",
      "  unmapped            : 152 cores\n"
     ]
    }
   ],
   "source": [
    "# build the new hierarchy\n",
    "hierarchy = build_hierarchy_map(features_2080)\n",
    "hierarchy.to_csv(\"../data/hierarchy_map.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095d3c08-e479-4609-a0b2-e991de47bb9f",
   "metadata": {},
   "source": [
    "#### add mode as minorness for both feature sets and update the mapping to include it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "29be81d6-12ad-4382-8d24-0edc58358da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1802 entries, 0 to 1801\n",
      "Data columns (total 5 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   song_id         1802 non-null   int64  \n",
      " 1   key             1802 non-null   object \n",
      " 2   mode            1802 non-null   object \n",
      " 3   key_confidence  1802 non-null   float64\n",
      " 4   bpm             1802 non-null   float64\n",
      "dtypes: float64(2), int64(1), object(2)\n",
      "memory usage: 70.5+ KB\n"
     ]
    }
   ],
   "source": [
    "mode_df = pd.read_parquet(\"../data/processed/audio_metadata.parquet\")\n",
    "mode_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c34ae262-4700-4fd0-aff3-bc81821ea49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add minorness_score to features. Mode: 'major'/'minor' or 0/1\n",
    "m = mode_df[['song_id', 'mode', 'key_confidence']].copy()\n",
    "    \n",
    "# Convert mode to binary (1=minor, 0=major)\n",
    "m['is_minor'] = (m['mode'].str.lower() == 'minor').astype(int)\n",
    "\n",
    "# Calculate minorness\n",
    "m['minorness_score'] = m['is_minor'] * m['key_confidence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f09d1431-cfeb-42bc-8f70-bb31df251e13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>song_id</th>\n",
       "      <th>mode</th>\n",
       "      <th>key_confidence</th>\n",
       "      <th>is_minor</th>\n",
       "      <th>minorness_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>minor</td>\n",
       "      <td>0.259305</td>\n",
       "      <td>1</td>\n",
       "      <td>0.259305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>minor</td>\n",
       "      <td>0.706457</td>\n",
       "      <td>1</td>\n",
       "      <td>0.706457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>minor</td>\n",
       "      <td>0.983184</td>\n",
       "      <td>1</td>\n",
       "      <td>0.983184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>major</td>\n",
       "      <td>0.997981</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>major</td>\n",
       "      <td>0.778527</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   song_id   mode  key_confidence  is_minor  minorness_score\n",
       "0        2  minor        0.259305         1         0.259305\n",
       "1        3  minor        0.706457         1         0.706457\n",
       "2        4  minor        0.983184         1         0.983184\n",
       "3        5  major        0.997981         0         0.000000\n",
       "4        7  major        0.778527         0         0.000000"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "98f057b8-d84d-49db-bfcd-a67bd2c03f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set song_id as index in m before merging\n",
    "m = m.set_index('song_id')\n",
    "# Merge\n",
    "features_2080 = features_2080.merge(m[['minorness_score']], left_index=True, right_index=True, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f3dac289-328b-4029-b065-7d3a0ad63a48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2081"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features_2080.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8008ecd1-fff0-4ef0-a73f-c011966e2994",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_3900 = features_3900.merge(m[['minorness_score']], left_index=True, right_index=True, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "86424a19-ba0a-4918-bc69-44f35c28fd5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3901"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features_3900.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "eb3d6e76-094c-4962-baba-742033d460f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# go up and re-run the cells that create the new decorrelated and pca datasets, then re-run all experiment notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "6cb899a5-90a5-4764-9a9c-bc0650483edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1802, 1692) (1802, 3512)\n",
      "(1802, 187) (1802, 187)\n",
      "(1802, 243) (1802, 318)\n"
     ]
    }
   ],
   "source": [
    "print(features_decorr_2080.shape, features_decorr_3900.shape)\n",
    "print(features_pca_grouped_2080.shape, features_pca_grouped_3900.shape)\n",
    "print(features_pca_global_2080.shape, features_pca_global_3900.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e601b2ba-6d92-471c-8e06-2ed6d13bee5c",
   "metadata": {},
   "source": [
    "The correlation reduction and the pca reduction both seem to reduce in very similar ways:\n",
    "- with pca per group being obvious that it preserves the same amount of features\n",
    "- decorr - 1800 to 1692 - 6% reduction | 3900 to 3512 - 10% reduction\n",
    "- pca global preserves only 13.5% of the initial dataset size for the 2080, and 8.1% for the 3900 one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "7d1a10ed-a0fd-43bd-8f0c-e778c15e44c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add mode to the newly created datasets too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "a2c69ca8-bb94-4081-bb32-afd1d60375ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_decorr_2080 = features_decorr_2080.merge(m[['minorness_score']], left_index=True, right_index=True, how='left')\n",
    "features_decorr_3900 = features_decorr_3900.merge(m[['minorness_score']], left_index=True, right_index=True, how='left')\n",
    "features_pca_grouped_2080 = features_pca_grouped_2080.merge(m[['minorness_score']], left_index=True, right_index=True, how='left')\n",
    "features_pca_grouped_3900 = features_pca_grouped_3900.merge(m[['minorness_score']], left_index=True, right_index=True, how='left')\n",
    "features_pca_global_2080 = features_pca_global_2080.merge(m[['minorness_score']], left_index=True, right_index=True, how='left')\n",
    "features_pca_global_3900 = features_pca_global_3900.merge(m[['minorness_score']], left_index=True, right_index=True, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "1a6d2f45-a93e-48a8-860e-74f6b2cb183a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1802, 1693) (1802, 3513)\n",
      "(1802, 188) (1802, 188)\n",
      "(1802, 244) (1802, 319)\n"
     ]
    }
   ],
   "source": [
    "print(features_decorr_2080.shape, features_decorr_3900.shape)\n",
    "print(features_pca_grouped_2080.shape, features_pca_grouped_3900.shape)\n",
    "print(features_pca_global_2080.shape, features_pca_global_3900.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c53ce17-9e17-4185-be44-7f5be219832d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf-gpu)",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
