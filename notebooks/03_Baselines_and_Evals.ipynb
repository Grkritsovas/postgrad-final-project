{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ee25e05-c571-4062-a49e-6b7d51ed5b18",
   "metadata": {},
   "source": [
    "## Check simple baselines (taking only a single row from the dynamic data, taking the most prominent one (q25)) and do a quick evaluation of the quality of the mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "878a388d-e5ea-4701-add7-7c029668fdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "# load utils\n",
    "from src.category_descriptor_selection import *\n",
    "from src.make_dataset.split_data import *\n",
    "from src.make_dataset import split_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "101399db-3065-427f-9122-3018f12e91db",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES_PATH = Path(\"../data/raw/features\")\n",
    "OUTPUT_PATH = Path(\"../data/processed\")\n",
    "OUTPUT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "HIERARCHY_PATH = Path(\"../data/hierarchy_map.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "661947ae-bb72-4ee5-b2d2-359e8a54dd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_3900 = pd.read_parquet(OUTPUT_PATH / \"features_3900.parquet\")\n",
    "hierarchy = pd.read_csv(HIERARCHY_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ad1ae060-fba6-4747-b56b-882b20869f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv5_splits = load_kfold_splits()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b6703c-6843-44b5-8cc8-983d35062ad5",
   "metadata": {},
   "source": [
    "### Baselines - q25, single row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b699ba5a-52cb-4450-8f16-7c18e6e3f339",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "def evaluate_baseline(X_baseline, y_valence, y_arousal, tr_ids, va_ids, name=\"Baseline\"):\n",
    "    \"\"\"Evaluate a baseline feature set on a single train/val split\"\"\"\n",
    "    \n",
    "    # Get train/val sets\n",
    "    tr_idx = X_baseline.index.intersection(tr_ids)\n",
    "    va_idx = X_baseline.index.intersection(va_ids)\n",
    "    \n",
    "    # Impute with train medians\n",
    "    med = X_baseline.loc[tr_idx].median(numeric_only=True)\n",
    "    Xtr = X_baseline.loc[tr_idx].fillna(med)\n",
    "    Xva = X_baseline.loc[va_idx].fillna(med)\n",
    "    \n",
    "    # Train and evaluate valence\n",
    "    rf_v = RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1)\n",
    "    rf_v.fit(Xtr, y_valence.loc[tr_idx])\n",
    "    pred_v = rf_v.predict(Xva)\n",
    "    rmse_v = np.sqrt(((y_valence.loc[va_idx] - pred_v)**2).mean())\n",
    "    r2_v = r2_score(y_valence.loc[va_idx], pred_v)\n",
    "    \n",
    "    # Train and evaluate arousal\n",
    "    rf_a = RandomForestRegressor(n_estimators=200, random_state=43, n_jobs=-1)\n",
    "    rf_a.fit(Xtr, y_arousal.loc[tr_idx])\n",
    "    pred_a = rf_a.predict(Xva)\n",
    "    rmse_a = np.sqrt(((y_arousal.loc[va_idx] - pred_a)**2).mean())\n",
    "    r2_a = r2_score(y_arousal.loc[va_idx], pred_a)\n",
    "    \n",
    "    # Joint metrics\n",
    "    joint_rmse = 0.5 * (rmse_v + rmse_a)\n",
    "    joint_r2 = 0.5 * (r2_v + r2_a)\n",
    "    \n",
    "    print(f\"\\n{name} Results:\")\n",
    "    print(f\"  Features: {X_baseline.shape[1]}\")\n",
    "    print(f\"  Valence RMSE: {rmse_v:.4f}, R²: {r2_v:.4f}\")\n",
    "    print(f\"  Arousal RMSE: {rmse_a:.4f}, R²: {r2_a:.4f}\")\n",
    "    print(f\"  Joint RMSE:   {joint_rmse:.4f}, R²: {joint_r2:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'valence_rmse': rmse_v, 'valence_r2': r2_v,\n",
    "        'arousal_rmse': rmse_a, 'arousal_r2': r2_a,\n",
    "        'joint_rmse': joint_rmse, 'joint_r2': joint_r2\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a869bba0-682c-4e4a-b0ba-c379f093a9b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q25 baseline: 260 features\n"
     ]
    }
   ],
   "source": [
    "# filter columns ending with _q25 (descriptor that carries on average the strongest signal)\n",
    "X_q25 = features_3900[[c for c in features_3900.columns if c.endswith('_q25')]]\n",
    "print(f\"Q25 baseline: {X_q25_only.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8c323274-fb3e-4de9-a071-265fc58d7930",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr, va, te = load_splits_triplet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4b4c247f-4001-4f40-8a1c-c1116e7ec9c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q25 Only Results:\n",
      "  Features: 260\n",
      "  Valence RMSE: 0.8301, R²: 0.5010\n",
      "  Arousal RMSE: 0.8293, R²: 0.6099\n",
      "  Joint RMSE:   0.8297, R²: 0.5555\n"
     ]
    }
   ],
   "source": [
    "esults_q25 = evaluate_baseline_single_split(X_q25, y_valence_mean, y_arousal_mean, tr, va, \"Q25 Only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "404ec57d-aa0e-4302-a9e2-394d8a30f820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean Only Results:\n",
      "  Features: 260\n",
      "  Valence RMSE: 0.8298, R²: 0.5014\n",
      "  Arousal RMSE: 0.8436, R²: 0.5964\n",
      "  Joint RMSE:   0.8367, R²: 0.5489\n"
     ]
    }
   ],
   "source": [
    "# Baseline 2: Mean only (the average signal from the 45 seconds)\n",
    "X_mean = features_3900[[c for c in features_3900.columns if c.endswith('_mean')]]\n",
    "results_mean = evaluate_baseline(X_mean, y_valence_mean, y_arousal_mean, tr, va, \"Mean Only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "4f04fd62-73b5-450a-b801-f4a98316c18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features_first_row_only(features_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Create features using only the first frame of each song\"\"\"\n",
    "    from src.io import load_opensmile_csv\n",
    "    from src.cleaning import clean\n",
    "    rows = []\n",
    "    ids = []\n",
    "    \n",
    "    csv_files = sorted(features_path.glob(\"*.csv\"))\n",
    "    \n",
    "    for csv_path in tqdm(csv_files, desc=\"Getting first frame only\"):\n",
    "        sid = int(csv_path.stem)\n",
    "        # Load and clean\n",
    "        df = load_opensmile_csv(csv_path, sep=';')\n",
    "        df_clean = clean(df)\n",
    "            \n",
    "        # Quality check\n",
    "        if df_clean.shape[0] < 1 or df_clean.shape[1] < 10:\n",
    "            continue\n",
    "            \n",
    "        # Just take first row, convert to Series\n",
    "        first_row = df_clean.iloc[0]\n",
    "            \n",
    "        rows.append(first_row)\n",
    "        ids.append(sid)\n",
    "    \n",
    "    # Create DataFrame from first rows\n",
    "    first_frame_df = pd.DataFrame(rows, index=ids)\n",
    "    first_frame_df.index.name = 'song_id'\n",
    "    first_frame_df.index = first_frame_df.index.astype(int)\n",
    "    first_frame_df = first_frame_df.sort_index()\n",
    "    \n",
    "    print(f\"Created first-frame dataset: {first_frame_df.shape}\")\n",
    "    \n",
    "    return first_frame_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "3ba8f3b4-2d23-4390-86b5-7ebcbb092452",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting first frame only: 100%|█████████████████████████████████████████████████████| 1802/1802 [00:59<00:00, 30.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created first-frame dataset: (1802, 260)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Baseline 3: first row\n",
    "X_first_row = create_features_first_row_only(FEATURES_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "944f8d11-1f3f-4e9d-b515-087dab543328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First row Only Results:\n",
      "  Features: 260\n",
      "  Valence RMSE: 1.0562, R²: 0.1922\n",
      "  Arousal RMSE: 1.0519, R²: 0.3724\n",
      "  Joint RMSE:   1.0541, R²: 0.2823\n"
     ]
    }
   ],
   "source": [
    "results_first_row = evaluate_baseline(X_first_row, y_valence_mean, y_arousal_mean, tr, va, \"First row Only\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68646b95-f38f-4387-bc78-e29f32c9bd63",
   "metadata": {},
   "source": [
    "### Recap:\n",
    "- [Q25] over Rows : Joint RMSE:   0.8297, R²: 0.5555\n",
    "- [Mean] over Rows: Joint RMSE:   0.8367, R²: 0.5489\n",
    "- [Single] Row only: Joint RMSE:   1.0541, R²: 0.2823"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f463ad-3f3e-4ebc-a6db-96a3b6469aef",
   "metadata": {},
   "source": [
    "### Evaluate Mapping quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "57ffbc31-eb3a-4475-89ad-3aa699503c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load labels\n",
    "labels = pd.read_parquet(\"../data/processed/core_dataset.parquet\")\n",
    "labels = labels.set_index(\"song_id\")\n",
    "labels.index = labels.index.astype(int)\n",
    "\n",
    "# Align features and labels\n",
    "common_ids = features_3900.index.intersection(labels.index)\n",
    "\n",
    "X = features_3900.loc[common_ids] # apply the alignment\n",
    "y_valence_mean = labels.loc[common_ids, \"valence_mean\"]\n",
    "y_arousal_mean = labels.loc[common_ids, \"arousal_mean\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "389cb369-d827-4758-83a2-cccfe1f57b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage on current design: 100.0% (260/260)\n",
      "Ambiguous features (mapped to >1 perceptual): 0\n"
     ]
    }
   ],
   "source": [
    "def mapping_coverage(hierarchy_map, agg_df):\n",
    "    bases_in_X = sorted({base_of(c) for c in agg_df.columns if '_PC' not in c})\n",
    "    mapped = set(hierarchy_map['feature'])\n",
    "    cov = len(set(bases_in_X) & mapped) / max(1, len(bases_in_X))\n",
    "    print(f\"Coverage on current design: {cov:.1%} ({len(set(bases_in_X)&mapped)}/{len(bases_in_X)})\")\n",
    "\n",
    "    amb = (hierarchy_map.groupby('feature')['perceptual'].nunique())\n",
    "    amb = amb[amb > 1]\n",
    "    print(f\"Ambiguous features (mapped to >1 perceptual): {len(amb)}\")\n",
    "    if len(amb): display(amb.head(15))\n",
    "\n",
    "mapping_coverage(hierarchy, features_3900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e730c0f-bb37-4e93-84dd-264c188e6d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "def group_ablation_cv(X, y, hierarchy_map, splits, n_estimators=300):\n",
    "    if any('_PC' in c for c in X.columns):\n",
    "        raise ValueError(\"Run on non-PCA (LLD) designs.\")\n",
    "\n",
    "    def cv_rmse(X_):\n",
    "        rmses=[]\n",
    "        for tr_ids, va_ids in splits:\n",
    "            tr = X_.index.intersection(tr_ids); va = X_.index.intersection(va_ids)\n",
    "            med = X_.loc[tr].median()\n",
    "            Xtr, Xva = X_.loc[tr].fillna(med), X_.loc[va].fillna(med)\n",
    "            ytr, yva = y.loc[tr], y.loc[va]\n",
    "            m = RandomForestRegressor(n_estimators=n_estimators, random_state=42, n_jobs=-1)\n",
    "            m.fit(Xtr, ytr); p = m.predict(Xva)\n",
    "            rmses.append(float(np.sqrt(((yva - p)**2).mean())))\n",
    "        return float(np.mean(rmses))\n",
    "\n",
    "    base2per = hierarchy_map.set_index('feature')['perceptual'].to_dict()\n",
    "    per2cols = {}\n",
    "    for c in X.columns:\n",
    "        g = base2per.get(base_of(c))\n",
    "        if g: per2cols.setdefault(g, []).append(c)\n",
    "\n",
    "    rmse_full = cv_rmse(X)\n",
    "    rows=[{'perceptual':'__FULL__','rmse':rmse_full,'delta':0.0,'dropped':0}]\n",
    "    for g, cols in per2cols.items():\n",
    "        rmse = cv_rmse(X.drop(columns=cols, errors='ignore'))\n",
    "        rows.append({'perceptual':g, 'rmse':rmse, 'delta':rmse - rmse_full, 'dropped':len(cols)})\n",
    "    return pd.DataFrame(rows).sort_values('rmse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "99bb4fd8-1f43-4769-a397-b8ebc01a1570",
   "metadata": {},
   "outputs": [],
   "source": [
    "ab_tbl = group_ablation_cv(features_3900, y_valence_mean, hierarchy, cv5_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "40e8c844-d425-44d7-9246-0bc2d8aa6c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            perceptual      rmse     delta  dropped\n",
      "4           dissonance  0.830694 -0.003224      480\n",
      "0             __FULL__  0.833918  0.000000        0\n",
      "2      tonal_stability  0.834852  0.000934      960\n",
      "6            minorness  0.835042  0.001124      240\n",
      "1        melodiousness  0.835718  0.001799      600\n",
      "3         articulation  0.836310  0.002392      660\n",
      "7  rhythmic_complexity  0.838138  0.004220      600\n",
      "5   rhythmic_stability  0.838147  0.004228      360\n"
     ]
    }
   ],
   "source": [
    "print(ab_tbl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9250e70-b270-4c8a-beb1-f727d3d3c6d7",
   "metadata": {},
   "source": [
    "### checking the new map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "67b74a87-7f82-4622-8184-92e1cc7431b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchy = pd.read_csv(HIERARCHY_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "db8ed5e8-4dc5-4b0d-a6c3-3d49a6ba74ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage on current design: 100.0% (260/260)\n",
      "Ambiguous features (mapped to >1 perceptual): 0\n"
     ]
    }
   ],
   "source": [
    "mapping_coverage(hierarchy, features_3900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ab06728b-91ce-407d-80be-b696b188a48f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>perceptual</th>\n",
       "      <th>musical</th>\n",
       "      <th>acoustic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>F0final_sma_amean</td>\n",
       "      <td>melodiousness</td>\n",
       "      <td>pitch</td>\n",
       "      <td>fundamental_freq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>F0final_sma_de_amean</td>\n",
       "      <td>melodiousness</td>\n",
       "      <td>pitch</td>\n",
       "      <td>fundamental_freq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F0final_sma_de_stddev</td>\n",
       "      <td>tonal_stability</td>\n",
       "      <td>pitch</td>\n",
       "      <td>pitch_delta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F0final_sma_stddev</td>\n",
       "      <td>tonal_stability</td>\n",
       "      <td>pitch</td>\n",
       "      <td>pitch_delta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>audSpec_Rfilt_sma[0]_amean</td>\n",
       "      <td>unmapped</td>\n",
       "      <td>other</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      feature       perceptual musical          acoustic\n",
       "0           F0final_sma_amean    melodiousness   pitch  fundamental_freq\n",
       "1        F0final_sma_de_amean    melodiousness   pitch  fundamental_freq\n",
       "2       F0final_sma_de_stddev  tonal_stability   pitch       pitch_delta\n",
       "3          F0final_sma_stddev  tonal_stability   pitch       pitch_delta\n",
       "4  audSpec_Rfilt_sma[0]_amean         unmapped   other           unknown"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hierarchy.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ae85f160-9299-41ef-80b5-117645cca6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "ab_tbl = group_ablation_cv(features_3900, y_valence_mean, hierarchy, cv5_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0f2d41c6-1820-4877-be53-51200cdd67a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            perceptual      rmse     delta  dropped\n",
      "5           dissonance  0.829552 -0.004366      240\n",
      "6  rhythmic_complexity  0.832560 -0.001358      450\n",
      "2        melodiousness  0.833146 -0.000773       60\n",
      "0             __FULL__  0.833918  0.000000        0\n",
      "1      tonal_stability  0.834195  0.000277      120\n",
      "7   rhythmic_stability  0.834645  0.000727       30\n",
      "3         articulation  0.836566  0.002648      720\n",
      "4             unmapped  0.852741  0.018823     2280\n"
     ]
    }
   ],
   "source": [
    "print(ab_tbl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c870623-a246-44dd-8a19-19a7933d9245",
   "metadata": {},
   "source": [
    "Here, dropping 158 unmapped features corresponding to 2280 base features * their descriptors, leads to only a 2% decrease in performance, validated across 5 folds, which means that the cost of dropping more features is minimal for interpretability (better mapping quality) and feature reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afc2871-03ce-4185-8eea-3dcbd722e721",
   "metadata": {},
   "source": [
    "### checking the old mapping quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6fa0a3d5-606b-472e-8e15-e742eb133d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    feature          perceptual  musical            acoustic\n",
      "                  jitterLocal_sma_de_stddev        articulation    voice              jitter\n",
      "                       pcm_zcr_sma_de_amean        articulation   rhythm       zero_crossing\n",
      "                       jitterDDP_sma_stddev        articulation    voice              jitter\n",
      "               audSpec_Rfilt_sma[25]_stddev          dissonance spectral        audspec_high\n",
      "              pcm_fftMag_mfcc_sma[13]_amean          dissonance   timbre           mfcc_high\n",
      "                audSpec_Rfilt_sma[25]_amean          dissonance spectral        audspec_high\n",
      "             audSpec_Rfilt_sma_de[13]_amean       melodiousness temporal   audspec_delta_mid\n",
      "             audSpec_Rfilt_sma_de[12]_amean       melodiousness temporal   audspec_delta_mid\n",
      "            audSpec_Rfilt_sma_de[14]_stddev       melodiousness temporal   audspec_delta_mid\n",
      "     pcm_fftMag_fband1000-4000_sma_de_amean           minorness   timbre        band_midhigh\n",
      "      pcm_fftMag_fband250-650_sma_de_stddev           minorness   timbre         band_lowmid\n",
      "               pcm_fftMag_mfcc_sma[5]_amean           minorness   timbre            mfcc_mid\n",
      "              audSpec_Rfilt_sma_de[5]_amean rhythmic_complexity temporal   audspec_delta_low\n",
      "              audSpec_Rfilt_sma_de[4]_amean rhythmic_complexity temporal   audspec_delta_low\n",
      "    pcm_fftMag_spectralEntropy_sma_de_amean rhythmic_complexity   timbre spectral_complexity\n",
      "                audSpec_Rfilt_sma[5]_stddev  rhythmic_stability spectral         audspec_low\n",
      "                 audSpec_Rfilt_sma[5]_amean  rhythmic_stability spectral         audspec_low\n",
      "                pcm_RMSenergy_sma_de_stddev  rhythmic_stability dynamics              energy\n",
      "pcm_fftMag_spectralRollOff25.0_sma_de_amean     tonal_stability   timbre    spectral_rolloff\n",
      "           pcm_fftMag_mfcc_sma_de[6]_stddev     tonal_stability   timbre        mfcc_general\n",
      "  pcm_fftMag_spectralHarmonicity_sma_stddev     tonal_stability  harmony         harmonicity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8825/298277679.py:6: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: g.sample(min(3, len(g)), random_state=0)))\n"
     ]
    }
   ],
   "source": [
    "assert hierarchy['feature'].is_unique\n",
    "assert not hierarchy[['perceptual','musical','acoustic']].isna().any().any()\n",
    "\n",
    "# human spot-check (3 exemplars per perceptual group)\n",
    "spot = (hierarchy.groupby('perceptual', group_keys=False)\n",
    "                     .apply(lambda g: g.sample(min(3, len(g)), random_state=0)))\n",
    "print(spot.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1594f0-2c37-488e-a2fa-2dd8621ef993",
   "metadata": {},
   "source": [
    "Assessment on the projects concerned mapping (musical and acoustic are just for quick analysis, perceptual is the one used throughout the project for linking the emotion labels into human-relevant categories that influence the emotion.\n",
    "Missed-messy mappings:\n",
    "#### Articulation(0/3):\n",
    "- jitterLocal_sma_de_stddev -> articulation | accurate mapping that can be found -> phonation/voice quality, weak link for articulation\n",
    "- pcm_zcr_sma_de_amean -> articulation | accurate mapping that can be found -> noisiness/percussiveness/brightness, weak link for articulation\n",
    "- jitterDDP_sma_stddev -> articulation | accurate mapping that can be found -> phonation stability, weak link for articulation\n",
    "#### Minorness(0/3).\n",
    "- pcm_fftMag_fband1000–4000_sma_de_amean -> minorness |  weak/highly speculative link for minorness\n",
    "- pcm_fftMag_fband250–650_sma_de_stddev -> minorness |  weak/highly speculative link for\n",
    "- pcm_fftMag_mfcc_sma[5]_amean-> minorness |  weak/highly speculative link for\n",
    "#### Dissonance(2/3)\n",
    "- pcm_fftMag_mfcc_sma[13]_amean -> dissonance | accurate mapping that can be found -> spectral envelope/timbre, weak link for dissonance\n",
    "#### Rhythmic_complexity(2/3)\n",
    "- pcm_fftMag_spectralEntropy_sma_de_amean -> rhythmic_complexity | accurate mapping that can be found -> tonality/noise-likeness, weak link for rhythmic complexity\n",
    "#### Tonal_Stability(1/3)\n",
    "- pcm_fftMag_spectralRollOff25.0_sma_de_amean -> tonal_stability | accurate mapping that can be found -> energy percentile/brightness, weak link for tonal stability\n",
    "- pcm_fftMag_mfcc_sma_de[6]_stddev -> tonal_stability | accurate mapping that can be found -> timbre , weak link for tonal stability\n",
    "#### Overall:\n",
    "High-band filtered spectrum -> dissonance rows plausible; increased high-freq energy/variance can relate to harshness/roughness, so they’re at least defensible without stronger evidence.\n",
    "\n",
    "Rhythmic_complexity/stability assignments for low-band (index ~5) energy/deltas also arguable.\n",
    "#### Overall the mapping should not be considered accurate and needs refinement.\n",
    "- Improvement idea: For the 7 perceptual categories, use a DL model that takes mel spectrograms as inputs and learns to predict them in a supervised manner, using the Mid-level perceptual musical features dataset (5000 songs annotated for 7 categories by musicians), then adapt it to the current dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e887a70-576e-4557-aff1-8f993c9b6647",
   "metadata": {},
   "source": [
    "### new map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8bd07ed1-c705-46bf-9618-16b47f00ef19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 feature          perceptual       musical             acoustic\n",
      "                  jitterDDP_sma_de_amean        articulation voice_quality               jitter\n",
      "          audSpec_Rfilt_sma_de[12]_amean        articulation        rhythm    audspec_delta_mid\n",
      "           audSpec_Rfilt_sma_de[9]_amean        articulation        rhythm audspec_delta_lowmid\n",
      "                       logHNR_sma_stddev          dissonance voice_quality          noise_ratio\n",
      "   pcm_fftMag_psySharpness_sma_de_stddev          dissonance        timbre            sharpness\n",
      "   pcm_fftMag_spectralCentroid_sma_amean          dissonance        timbre    spectral_centroid\n",
      "         voicingFinalUnclipped_sma_amean       melodiousness         voice              voicing\n",
      "      voicingFinalUnclipped_sma_de_amean       melodiousness         voice              voicing\n",
      "                    F0final_sma_de_amean       melodiousness         pitch     fundamental_freq\n",
      "                         minorness_score           minorness         other              unknown\n",
      "           audSpec_Rfilt_sma_de[1]_amean rhythmic_complexity        rhythm    audspec_delta_low\n",
      "                   pcm_zcr_sma_de_stddev rhythmic_complexity        rhythm        zero_crossing\n",
      "         audSpec_Rfilt_sma_de[24]_stddev rhythmic_complexity        rhythm   audspec_delta_high\n",
      "              pcm_RMSenergy_sma_de_amean  rhythmic_stability      dynamics               energy\n",
      "                 pcm_RMSenergy_sma_amean  rhythmic_stability      dynamics               energy\n",
      "     voicingFinalUnclipped_sma_de_stddev     tonal_stability         voice              voicing\n",
      "pcm_fftMag_spectralEntropy_sma_de_stddev     tonal_stability        timbre  spectral_complexity\n",
      "                      F0final_sma_stddev     tonal_stability         pitch          pitch_delta\n",
      "        pcm_fftMag_mfcc_sma_de[13]_amean            unmapped         other              unknown\n",
      "              audSpec_Rfilt_sma[6]_amean            unmapped         other              unknown\n",
      "   pcm_fftMag_fband250-650_sma_de_stddev            unmapped         other              unknown\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8825/298277679.py:6: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: g.sample(min(3, len(g)), random_state=0)))\n"
     ]
    }
   ],
   "source": [
    "assert hierarchy['feature'].is_unique\n",
    "assert not hierarchy[['perceptual','musical','acoustic']].isna().any().any()\n",
    "\n",
    "# human spot-check (3 exemplars per perceptual group)\n",
    "spot = (hierarchy.groupby('perceptual', group_keys=False)\n",
    "                     .apply(lambda g: g.sample(min(3, len(g)), random_state=0)))\n",
    "print(spot.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7242dee-c8ac-4e94-b097-1e03e1fcc74a",
   "metadata": {},
   "source": [
    "Some mappings still seem highly speculative.The current rule-based approach lacks the sophistication to capture perceptual-acoustic relationships accurately.\n",
    "#### Future work:\n",
    "##### Implement a supervised deep learning approach:\n",
    "\n",
    "- Training: Use the Mid-level Perceptual Musical Features dataset (5000 expert-annotated songs)\n",
    "- Architecture: CNN/Transformer on mel-spectrograms for direct perceptual prediction\n",
    "- Adaptation: Transfer learning to current dataset maintaining learned perceptual representations\n",
    "- Validation: Cross-reference with current mappings to retain high-confidence associations\n",
    "\n",
    "This would replace brittle rules with learned representations that better capture the complex, non-linear relationships between acoustic features and perceptual dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c3116b-1649-4f9d-a2fb-91699b5deea1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf-gpu)",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
